"use client";

import Footer from "../footer/page";
import "./next.css";
import React from "react";

const articles = [
  {
    title: "The Road to Artificial General Intelligence",
    description:"Artificial general intelligence (AGI) is a holy grail for many scientists of artificial intelligence. In recent years, a large community of researchers has emerged, focusing on the original ambitious goals of the AI field—creating and studying software or hardware systems with a general intelligence similar to, and perhaps even greater than, that of human beings. Kurzweil (1999) contrasted “narrow AI” with “strong AI,” using the former to refer to the creation of systems that perform specific intelligent behaviors in specific contexts, and the latter, essentially, referring to what is known as AGI. However, it is widely accepted that people do not have an undefined generality of intelligence, given the limits of realistic space and time, and for similar reasons, no real-world system will have an undefined generality. Human intelligence combines a certain generality of the field with several highly specialized aspects that are designed to efficiently solve pragmatic problems, and real AGI systems will combine generality and specificity in their own way.I didnt write that paragraph. An AI did. I went to ai-writer.com and plugged the phrase “artificial general intelligence” into the query box. A few minutes later, the AI delivered to my email inbox a roughly 400-word article based on nearly two dozen web-based sources that the AI identified itself. The paragraph above is the first paragraph of that story, with a bit of editing (for one thing, the ai-writer AI is evidently British). Its impressiveBut Im not worried about my job, as an editor or a writer. Because actually, this paragraph is little more than a somewhat sophisticated cut-and-paste job. The interface even has a slider at the top, where you can adjust how much of the original text is rewritten. It defaults to about 50 percent, and it wont allow rewrite levels above about 70 percent. Essentially, the output is a montage of key words and phrases from sources connected by fill text and the occasional direct quote—just a step above your basic undergraduate plagiarism. In fact, the site offers a “Text Rewording” service that, to my mind, verges perilously on that undergraduate plagiarism: the AI will rework text you copy and paste into it “from various sources” to create “unique content.Its difficult to balance this example with the dark pessimism of thinkers like Nick Bostrom, whose 2014 book Superintelligence considers the implications of a runaway AI—one that decides it doesnt need humans around anymore. Or with Stephen Hawkings warning, in a BBC interview reported by Rory Cellan-Jones, that an advanced AI “could spell the end of the human race.” Elon Musk is similarly alarmed, according to an insightful MIT Technology Review article by Paul FordIt may be even more difficult to square this simplistic output with the unbounded optimism of thinkers like Ray Kurzweil, who anticipates true machine intelligence within the next 10 years or so, and an AI-powered utopia within the centurySo how long will it be before we can welcome our new computer overlords? (And when can I put down my pen?) The answer to that question … dependsFirst, we need some definition of what we mean. Kris Hammond, cofounder of Narrative Science and a professor of computer science, offers a good set of basic definitions in a 2015 article for Computerworld. For Hammond, “artificial intelligence” is simply a subfield of computer science focused on developing computers that can do things people typically do. As Hammond points out, the focus of this definition is output, not process—“it is AI if it is smart, but it doesnt have to be smart like us.”Hammond then differentiates between “weak AI,” whose focus is “just getting systems to work,” and “strong AI,” which aims to actually simulate human reasoning. Thats the difference between something like todays most sophisticated neural networks and deep learning systems and IBMs Deep Blue, which beat humans at chess using a completely inhuman brute-force computing approach.Most of the action today, Hammond says, is in the middle ground between these two extremes. That ground is inhabited by systems that are inspired by human reasoning but dont aim to replicate it—for instance, IBM Watson. Hammond draws another distinction, as well, between AI designed to accomplish specific tasks (“narrow AI”) and systems that can engage in general reasoning (“general AI”)Most current applications are narrow AI, designed to address a specific set of problems or perform a limited set of tasks in a specific context. Some of those applications have exceeded human levels of performance within their fields—in some cases by magnitudes. One area where this is true is in games. DeepMinds AlphaGo has beaten human master players of Go, and the next generation, AlphaGo Zero, has beaten its predecessor. Chess-playing computers have been a staple of the AI game since the inception of the field; today, a smartphone app can offer grandmaster-level competition. Its impressive, but its also important to remember the core attribute of games: they are rule driven and controlled. Complex strategic games may involve thousands of possible moves, but all of the pieces are visible—as Yoav Shoham and colleagues point out in their 2017 report on the state of AI, its a perfect experimental environment for testing an AIAnd the performance of even the most impressive machines degrades rapidly outside the environments for which they were created. As an example, Shohams report offers, “a human who can read Chinese characters would likely understand Chinese speech, know something about Chinese culture and even make good recommendations at Chinese restaurants. In contrast, very different AI systems would be needed for each of these tasks.” Systems in Kris Hammonds middle ground suffer a similar fate: Siri or Alexa can field a number of queries and perform a wide range of tasks, from finding the movie you want to finding a nearby Thai restaurant to getting you directions to that restaurant. But when they receive an input they dont expect, they cant reason through to the desired result. Instead, they deliver nonsense or worseThe key performance measure for true AGI varies, but the general standard, in the words of Luke Muehlhauser from the Machine Intelligence Research Institute, is that it will “have the capacity to solve somewhat-arbitrary problems in somewhat-arbitrary environments” and be able to apply learning across domains. Muehlhauser catalogs four tests that try to operationalize AGI; the best known is the Turing test—the AI will be able to fool a human into believing it is conversing with another human. Other tests focus on the machines ability to carry out a practical task—such as making coffee—in a new environment or graduate from collegeThe problem with narrow AIs is not that they are narrow, according to Peter Voss, but that they are inherently narrow—they cant adapt and grow. Their capabilities are “frozen in time,” limited to the solutions and approaches bestowed on them by their programmers. They are not true intelligence, but only “frozen bits of human ingenuity.” Nor is narrow AI a step on the road to general intelligence, according to Voss. Rather, reaching true AGI will require a fundamentally different approach, one that can imbue the machine with cross-domain learning, deep reasoning abilities, and even common sense. For Voss, some of the barriers to reaching AGI are commercial—the market demand is for solutions that can be deployed right now and provide results immediately. Few companies have the vision to pursue the long game of true AGI.",
    image: "/agi.jpg", 
  },
  {
    title: "What is generative AI?",
    description:"Generative AI describes a type of artificial intelligence that creates new content in forms such as text, images, audio, music, or code, similar to what humans can produce. These systems learn from large datasets and go beyond traditional machine learning, which usually focuses on making predictions. Instead, generative AI identifies patterns and structures within data to generate entire pieces of new content, like paragraphs instead of just single words.Generative AI has drawn considerable interest in business applications, including marketing, software development, and design. Albert Ziegler from GitHub highlights that generative AIs impact will likely reach all industries, even influencing everyday tasks like logo design, business advice, and tax preparation.In business, generative AI is useful across various domains:Software Development: Assisting with coding, editing, and testing.Content Creation: Personalizing product descriptions and ad copy.Design Creation: Generating layouts and assisting with graphic design.Video Creation: Building and enhancing videos and images.Language Translation: Translating communications across languages.Personalization: Tailoring products and services for individual customers.Operations: Streamlining supply chain management and pricing.For developers, generative AI significantly boosts productivity, helping them code and release software more efficiently.",  
      image: "/genai.jpg", 
  },
  {
    title: "What is retrieval-augmented generation, and what does it do for generative AI?",
    description:"One of the hottest topics in AI right now is RAG, RAG, or retrieval-augmented generation, is a trending method in AI for improving the relevance and accuracy of AI responses. RAG enables AI tools to access and use proprietary data without the need for extensive custom model training, keeping the models updated with new information. Without RAG, AI tools can only generate responses based on their original training data. With RAG, however, they can pull from a private database, enhancing their responses with newer data.Idan Gazit, Senior Director of Research at GitHub Next, and Software Engineer Colin Merkel explain that a model can be limited by the knowledge cut-off date in its training data. This is where RAG comes in, allowing models to use current data sources and produce more relevant outputs.Organizations that want to adapt AI for specific tasks generally use RAG or fine-tuning. Fine-tuning requires altering the model internal settings to perform highly specialized tasks, especially useful for custom codebases or niche language needs. RAG, by contrast, does not involve model changes; it simply retrieves and integrates information from a database to better tailor responses. Some companies use RAG initially and then fine-tune models as needed, while others find RAG alone is sufficient.Context is essential for AI tools to give helpful responses, just like for humans making informed decisions. Large language models (LLMs) today, typically transformers, operate within context windows—a defined data capacity that will likely expand with advancements. For instance, GitHub Copilot uses a technique called Fill-in-the-Middle (FIM), recognizing both the code before and after the cursor to provide more contextually relevant coding suggestions. It also pulls information from other open tabs to further improve response relevance, prioritizing the most recently accessed ones.A key challenge for machine learning engineers is selecting and organizing the input data within this context window to ensure the AI delivers the best possible response, a task known as prompt engineering",
    image: "/rag.jpg", 
  },
  {
    title: "How AI code generation works",
    description:"Generative AI coding tools are transforming software production for enterprises. They are not just used for code generation; they help detect vulnerabilities, assist in understanding unfamiliar codebases, streamline documentation, and improve pull request descriptions. This change is fundamentally reshaping how developers approach infrastructure, deployment, and even their work experiences.oWe are witnessing a significant turning point. As AI models continue to improve, avoiding adoption would be like asking an office worker to use a typewriter instead of a computer, says Albert Ziegler, principal researcher and a member of the GitHub Next research and development team.In this discussion, we will explore AI code generation, understanding how it works, its benefits, and how developers can leverage it to enhance their work experience and drive enterprise success in todays competitive landscape.How to Use AI to Generate CodeAI code generation involves machines generating partial or full lines of code instead of human developers. This emerging technology uses advanced machine learning models, specifically large language models, to grasp and replicate the syntax, patterns, and paradigms found in human-written code.The AI models behind these tools, such as ChatGPT and GitHub Copilot, are trained on natural language text and source code from publicly available sources that include a wide range of coding examples. This training enables these models to understand the nuances of different programming languages, coding styles, and best practices, allowing them to generate code suggestions that are both syntactically accurate and contextually relevant based on developers input.Used by 55 percent of developers, GitHub Copilot is our AI-powered pair programmer, providing contextual coding assistance based on your organizations codebase across multiple programming languages and is suitable for developers of all experience levels. With GitHub Copilot, developers can utilize AI to generate code in three primary ways:1. Typing Code and Receiving AutocompletionsAutocompletions represent the earliest form of AI code generation. John Berryman, a senior researcher in machine learning on the GitHub Copilot team, describes the experience: “I will be writing code and pause to think. While I am doing that, the assistant is also processing, observing surrounding code and open tabs. Then, it appears on the screen as gray ghost text that I can either reject, partially accept, or fully accept, modifying if needed.”This feature can benefit all developers, but experienced programmers often find it speeds them up significantly. “In many cases, especially for experienced programmers in familiar environments, this suggestion is a timesaver. I would have written the same thing; it is just faster to hit tab than to type those 20 characters myself,” says Johan Rosenkilde, principal researcher for GitHub Next.For developers, regardless of experience level, working in unfamiliar languages can be challenging. GitHub Copilots code completion suggestions can ease this. Berryman mentions, “Using GitHub Copilot for code completion has accelerated my learning. I often accept suggestions that I would not have thought of myself due to lack of familiarity with the syntax.”Becoming adept at using AI coding tools is a skill in itself; the more developers practice, the faster they become.2. Writing Explicit Comments to Get Better Code SuggestionsFor experienced developers navigating unfamiliar environments, tools like GitHub Copilot can help refresh their memories.Suppose a developer imports a new library they have not used before or struggles to recall a specific function or argument order. In such cases, making GitHub Copilot more aware of their intentions by adding comments can be beneficial.“It is likely that a developer may not remember the formula but can recognize it when GitHub Copilot suggests it based on a prompt,” says Rosenkilde. Natural language commentary can serve as a shortcut, especially when developers need to describe intent but are struggling with initial syntax.Naming functions and variables clearly and writing documentation can further improve suggestions. GitHub Copilot can interpret variable names, using them to understand what the function should achieve.This encourages better coding practices, as well-named variables and functions make code more maintainable. Often, a programmers primary task is maintaining code rather than writing it from scratch.“When you push that code, someone is going to review it, and they will appreciate it if the code has clear names and documentation,” says Rosenkilde. The collaboration between the developer and the AI tool is beneficial not only for the developer but also for the entire team.3. Directly Chatting with AIWith AI chatbots, code generation can become interactive. For instance, GitHub Copilot Chat enables developers to engage with the AI by asking it to explain code, improve syntax, provide ideas, generate tests, and modify existing code, making it a versatile tool for managing various coding tasks.Rosenkilde uses GitHub Copilots different functionalities:“When I need to do something and I am unsure how to start, I type the first few letters, then wait to see if Copilot can figure it out,” he says. “If that does not work, I might delete those characters and write a one-liner comment, then see if Copilot suggests the next line. If that also does not help, I switch to Copilot Chat and explain in detail what I need.”Copilot Chat typically provides more comprehensive responses than standard code completion. “It describes what needs to be done and how to do it, offering code examples. You can then respond and adjust, saying, ‘I see what you are aiming for, but I meant it differently,’” Rosenkilde adds.That said, using AI chatbots does not mean developers should rely on them entirely. Mistakes in reasoning can lead the AI to make further mistakes if unchecked. Berryman suggests interacting with the AI assistant similarly to pair programming with a human. “Go back and forth with it, explaining your task, asking for ideas, requesting help with code, and offering feedback. Guide the assistant to ensure it stays on track,” he recommends",
    image: "/gen work.jpg", // Update with your image path
  },
  {
    title: "A developers guide to prompt engineering and LLMs",
    description:"In a blog post from back in 2011, Marc Andreessen warned that “Software is eating the world.” Over a decade later, we are seeing a new type of technology emerging with even greater speed: generative artificial intelligence (AI). This new kind of AI includes a unique class of large language models (LLMs) that come from a decade of groundbreaking research and are capable of outperforming humans at certain tasks. Plus, you do not need a PhD in machine learning to build with LLMs—developers are already creating software with LLMs using basic HTTP requests and natural language prompts.In this article, we will share GitHubs journey with LLMs to help other developers learn how to best make use of this technology. The post is divided into two main sections: the first part describes how LLMs work at a high level and how to build applications using them, while the second focuses on a specific example of an LLM-based application—GitHub Copilot code completions.While others have done an impressive job documenting our work from the outside, we are excited to give you a peek into some of the thought processes behind the ongoing success of GitHub Copilot.Let us get started.Everything you need to know about prompt engineering in 1600 tokens or lessYou know when you are typing a text message on your phone, and just above the keypad there is a button suggesting the next word? That is basically what an LLM does—but on a much larger scale.Instead of predicting the next word for your text message, an LLM predicts the next best group of letters, called “tokens.” Similar to how you can keep tapping that middle button to complete your message, an LLM completes a document by predicting the next word. It will continue this process until it reaches a maximum number of tokens or encounters a special token that signals “Stop! This is the end of the document.”There is a key difference though. The language model on your phone is fairly simple—it asks, “Based only on the last two words entered, what is the most likely next word?” By contrast, an LLM produces an output more like “Based on the full content of every document ever known to exist in the public domain, what is the most likely next token in your document?” By training a well-designed model on such a huge dataset, an LLM can almost seem to have common sense, like understanding that a glass ball sitting on a table might roll off and shatter.But be cautious: LLMs sometimes confidently produce information that is not real or accurate, known as “hallucinations” or “fabulations.” LLMs can also seem to learn how to do things they were not initially trained to do. Historically, natural language models were created for specific tasks, like classifying the sentiment of a tweet, extracting business entities from an email, or finding similar documents, but now you can ask AI tools like ChatGPT to do tasks they were never specifically trained to do.Building applications using LLMsA document completion engine is just one of the many LLM applications that are appearing every day, ranging from conversational search, writing assistants, and automated IT support, to code completion tools like GitHub Copilot. But how can so many tools come from what is basically a document completion tool? The answer is that any application using an LLM is actually mapping between two domains: the user domain and the document domain",
    image: "/llm.jpg", // Update with your image path
  },
  {
    title: "Pittsburgh trades steel for AI tech",
    description:"Through two collaborative technology centers with Nvidia, Carnegie Mellon University and the University of Pittsburgh will expedite innovation and public-private collaboration.Nvidia is starting its first AI Tech Community in Pittsburgh, Pennsylvania, which will act as a link between academic institutions, business, and government organizations to collaborate on AI innovation.Announcing the new Nvidia AI Tech Community effort today at the Nvidia AI Summit in Washington, D.C., the program includes partnerships with Carnegie Mellon University and the University of Pittsburgh, as well as startups, businesses, and organizations situated in the city of bridges.The initiative's goal is to accelerate public-private collaborations in communities that have a lot of potential for utilizing AI to enable technological transformation. To access knowledge, two Nvidia joint technology centers will be built in Pittsburgh.By providing the newest technologies to researchers, staff, and students in higher education, Nvidia's Joint Center with Carnegie Mellon University (CMU) for Robotics, Autonomy, and AI will foster innovation in the domains of robotics and artificial intelligence. Additionally, the University of Pittsburgh and Nvidia's Joint Center for AI and Intelligent Systems will concentrate on computational prospects in the health sciences, including biomanufacturing and clinical medicine uses of AI.According to the U.S. News & World Report, CMU is the top AI university in the country. It has led the way in research on natural language processing and driverless cars. As the largest university-affiliated robotics research organization in the world, CMU's Robotics Institute unites over a thousand faculty, staff, students, post-doctoral fellows, and guests to use robotics to address the most difficult problems facing humanity.As an R1 research university at the vanguard of innovation, the University of Pittsburgh ranks sixth among U.S. universities in terms of research funding from the National Institutes of Health, surpassing $1 billion in research expenditures in fiscal year 2022 and ranking fourteenth among U.S. universities awarded utility patents. Nvidia will supply the centers with Jetson for robotics edge computing, Omniverse for simulation, and DGX for AI training.",
    image: "/aicompany.png", // Update with your image path
  },
];

const NextBlog = () => {
  return (
    <div className="container">
      <div className="article-grid">
        {articles.map((article, index) => (
          <div className="article" key={index}>
            <h2 className="article-title">{article.title}</h2>
            <img src={article.image} alt={article.title} />
            <div className="article-content">
              
              <p className="article-description">{article.description}</p>
            </div>
          </div>
        ))}
      </div>
      <Footer />
    </div>
  );
};

export default NextBlog;
